import asyncio
import time
import os
import json
import mimetypes
import re
import shutil
import urllib.parse
from PIL import Image
import aiohttp
import aiofiles
from pyrogram import Client
from pyrogram.types import InlineKeyboardMarkup, InlineKeyboardButton
import aria2p
from plugins.config import Config
from utils.shared import WEBAPP_PROGRESS, get_http_session

PROGRESS_UPDATE_DELAY = 1  # seconds between progress edits

# Global dictionary used by the Flask Mini App to read live progress percentages
# Replaced by Config.WEBAPP_PROGRESS for thread-safe singleton access



def _get_ffmpeg_bin() -> str:
    """Return the actual ffmpeg binary path, checking FFMPEG_PATH and PATH."""
    path = Config.FFMPEG_PATH  # could be 'ffmpeg' or '/usr/bin/ffmpeg'
    # If it already looks like a binary (has no dir separators), use shutil.which
    if os.sep not in path and '/' not in path:
        found = shutil.which(path)
        if found:
            return found
    if os.path.isfile(path):
        return path
    # Last resort: try well-known locations
    for candidate in ["/usr/bin/ffmpeg", "/usr/local/bin/ffmpeg"]:
        if os.path.isfile(candidate):
            return candidate
    return path  # return whatever was configured, let the caller fail


def _get_ffmpeg_dir() -> str:
    """Return the DIRECTORY containing ffmpeg ‚Äî what yt-dlp expects for ffmpeg_location."""
    return os.path.dirname(_get_ffmpeg_bin()) or None


def _get_ffprobe_bin() -> str:
    """Return the ffprobe binary path (same dir as ffmpeg)."""
    ffmpeg_dir = os.path.dirname(_get_ffmpeg_bin())
    ffprobe = os.path.join(ffmpeg_dir, "ffprobe") if ffmpeg_dir else "ffprobe"
    if os.path.isfile(ffprobe):
        return ffprobe
    found = shutil.which("ffprobe")
    return found or "ffprobe"

# ‚îÄ‚îÄ Streaming / HLS detection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Extensions that indicate a playlist / stream, not a direct media file
STREAMING_EXTENSIONS: dict[str, str] = {
    ".m3u8": ".mp4",
    ".m3u":  ".mp4",
    ".mpd":  ".mp4",   # DASH manifest
    ".ts":   ".mp4",   # raw MPEG-TS segment
}

HLS_MIME_TYPES = {
    "application/vnd.apple.mpegurl",
    "application/x-mpegurl",
    "application/dash+xml",
    "audio/mpegurl",
    "audio/x-mpegurl",
    "video/mp2t",
}


def needs_ffmpeg_download(url: str, mime: str) -> bool:
    """Return True if this URL must be downloaded with ffmpeg instead of aiohttp."""
    ext = os.path.splitext(urllib.parse.urlparse(url).path)[1].lower()
    return ext in STREAMING_EXTENSIONS or (mime or "").lower() in HLS_MIME_TYPES

async def resolve_url(url: str) -> str:
    """Resolve redirecting URLs (like reddit shortlinks) and bypass Twitter NSFW blocks."""
    # 1. Resolve Reddit short links
    if "redd.it" in url or "/s/" in url.lower():
        for _ in range(2): # 2 retries
            try:
                session = await get_http_session()
                async with session.head(
                    url, 
                    allow_redirects=True, 
                    timeout=10, 
                    proxy=Config.PROXY
                ) as resp:
                    url = str(resp.url)
                    break
            except Exception:
                await asyncio.sleep(1)

    # 2. Extract Twitter direct media URL if it's a tweet link
    if any(domain in url.lower() for domain in ["twitter.com", "x.com", "t.co"]):
        # Resolve t.co first if necessary
        if "t.co" in url.lower():
            for _ in range(2):
                try:
                    session = await get_http_session()
                    async with session.head(
                        url, 
                        allow_redirects=True, 
                        timeout=10, 
                        proxy=Config.PROXY
                    ) as resp:
                        url = str(resp.url)
                        break
                except Exception:
                    await asyncio.sleep(1)
                
        # Now Check for twitter.com / x.com and try vxtwitter API
        match = re.search(r'(?:twitter\.com|x\.com)/(?:[^/]+/status/|status/|status/|/)([0-9]+)', url, re.IGNORECASE)
        if match:
            tweet_id = match.group(1)
            api_url = f"https://api.vxtwitter.com/x/status/{tweet_id}"
            for _ in range(2):
                try:
                    session = await get_http_session()
                    async with session.get(api_url, timeout=10, proxy=Config.PROXY) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            media_urls = data.get("mediaURLs", [])
                            if media_urls:
                                # We return the direct raw video/image URL instead of the twitter page!
                                return media_urls[0]
                    break
                except Exception:
                    await asyncio.sleep(1)

    return url


def smart_output_name(filename: str) -> str:
    """
    Remap known streaming extensions to the proper container extension.
    e.g. 'stream.m3u8' ‚Üí 'stream.mp4'
    """
    stem, ext = os.path.splitext(filename)
    return stem + STREAMING_EXTENSIONS.get(ext.lower(), ext)

# ‚îÄ‚îÄ yt-dlp integration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

try:
    import yt_dlp
    YTDLP_AVAILABLE = True
except ImportError:
    YTDLP_AVAILABLE = False

_YTDLP_EXTRACTORS = None

def _get_ytdlp_extractors():
    """Lazy-load and cache the yt-dlp extractors (excluding GenericIE fallback)."""
    global _YTDLP_EXTRACTORS
    if _YTDLP_EXTRACTORS is None:
        try:
            from yt_dlp.extractor import gen_extractors
            _YTDLP_EXTRACTORS = [e for e in gen_extractors() if e.IE_NAME != 'generic']
        except Exception as e:
            Config.LOGGER.error(f"Failed to load yt-dlp extractors: {e}")
            _YTDLP_EXTRACTORS = []
    return _YTDLP_EXTRACTORS

# Domains where yt-dlp should be used directly. Additions here bypass dynamic extract checks and Regex strictness.
YTDLP_DOMAINS = {
    "youtube.com", "youtu.be", "youtube-nocookie.com",
    "instagram.com",
    "twitter.com", "x.com", "t.co",
    "tiktok.com", "vm.tiktok.com",
    "facebook.com", "fb.watch", "fb.com",
    "reddit.com", "v.redd.it", "redd.it",
    "dailymotion.com", "dai.ly",
    "vimeo.com",
    "twitch.tv", "clips.twitch.tv",
    "soundcloud.com",
    "bilibili.com", "b23.tv",
    "pinterest.com", "pin.it",
    "streamable.com",
    "rumble.com",
    "odysee.com",
    "bitchute.com",
    "mixcloud.com",
}

# Domains where cobalt API can be used as an alternative/fallback
COBALT_DOMAINS = {
    "youtube.com", "youtu.be",
    "reddit.com", "v.redd.it", "redd.it",
}


def is_ytdlp_url(url: str) -> bool:
    """Return True if the URL belongs to a yt-dlp-supported platform dynamically."""
    if not YTDLP_AVAILABLE:
        return False
    try:
        host = urllib.parse.urlparse(url).netloc.lower().lstrip("www.")
        # Step 1: Check Hardcoded fallback domains (handles shortened links like `t.co` and `fb.com/share`)
        if any(host == d or host.endswith("." + d) for d in YTDLP_DOMAINS):
            return True
        # Step 2: Dynamically query all yt-dlp extractors natively supported
        extractors = _get_ytdlp_extractors()
        return any(e.suitable(url) for e in extractors)
    except Exception:
        return False


def is_cobalt_url(url: str) -> bool:
    """Return True if the URL can be handled by cobalt API as a fallback."""
    if not Config.COBALT_API_URL:
        return False
    try:
        host = urllib.parse.urlparse(url).netloc.lower().lstrip("www.")
        return any(host == d or host.endswith("." + d) for d in COBALT_DOMAINS)
    except Exception:
        return False


def cancel_button(user_id: int) -> InlineKeyboardMarkup:
    """Build a simple cancel button markup."""
    return InlineKeyboardMarkup([[
        InlineKeyboardButton("‚úñÔ∏è Cancel", callback_data=f"cancel:{user_id}")
    ]])


async def _safe_edit(msg, text: str, reply_markup=None):
    """Edit a Telegram message, silently ignoring all errors."""
    try:
        await msg.edit_text(text, reply_markup=reply_markup)
    except Exception:
        pass


async def external_extract_youtube(url: str) -> dict | None:
    """
    Call the external Koyeb API for YouTube metadata extraction.
    Used as a fallback when local yt-dlp is blocked.
    """
    if not Config.YT_API_URL:
        return None
    
    api_url = f"{Config.YT_API_URL.rstrip('/')}/extract"
    try:
        session = await get_http_session()
        # High timeout (120s) because the external API uses WARP + Cold Boot
        async with session.post(api_url, json={"url": url}, timeout=120) as resp:
            if resp.status == 200:
                data = await resp.json()
                Config.LOGGER.info(f"Successfully extracted YouTube metadata via external API: {url}")
                return data
            else:
                Config.LOGGER.error(f"External YouTube API failed with status {resp.status}")
    except Exception as e:
        Config.LOGGER.error(f"External YouTube API exception: {e}")
    return None


async def fetch_ytdlp_title(url: str) -> str | None:
    """
    Extract the video title from yt-dlp (no download).
    Returns a clean filename like 'My Video Title.mp4', or None on failure.
    """
    if not YTDLP_AVAILABLE:
        return None
        
    # PRIMARY FOR YOUTUBE: Try external API first to bypass bot detection
    if "youtube.com" in url or "youtu.be" in url:
        Config.LOGGER.info(f"Primary YouTube title extraction via external API: {url}")
        info = await external_extract_youtube(url)
        if info:
            title = info.get("title") or info.get("id") or "video"
            title = re.sub(r'[\\/*?"<>|:\n\r\t]', "_", title).strip()
            ext = info.get("ext") or "mp4"
            return f"{title[:80]}.{ext}"

    loop = asyncio.get_running_loop()

    def _fetch():
        try:
            opts = {
                "quiet": True,
                "no_warnings": True,
                "skip_download": True,
                "format_sort": ["res", "vbr", "tbr", "fps"],
                "force_ipv4": True, # Common fix for Connection Reset on VPS
                "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "extractor_args": {
                    "youtube": {
                        "player_client": ["web", "creator"],
                    },
                    "youtubepot-bgutilhttp": {
                        "base_url": ["http://localhost:4416"],
                    }
                }
            }
            if Config.COOKIES_FILE and os.path.exists(Config.COOKIES_FILE):
                opts["cookiefile"] = Config.COOKIES_FILE
            if Config.PROXY:
                opts["proxy"] = Config.PROXY

            with yt_dlp.YoutubeDL(opts) as ydl:
                info = ydl.extract_info(url, download=False)
                title = info.get("title") or info.get("id") or "video"
                title = re.sub(r'[\\/*?"<>|:\n\r\t]', "_", title).strip()
                ext = info.get("ext") or "mp4"
                return f"{title[:80]}.{ext}"
        except Exception:
            return None

    return await loop.run_in_executor(None, _fetch)


async def fetch_http_filename(url: str, default_name: str = "downloaded_file") -> str:
    """
    Probe a direct URL with a HEAD request to extract the true filename from Content-Disposition
    or guess the extension from the Content-Type.
    """
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
    }
    try:
        session = await get_http_session()
        async with session.head(
            url, allow_redirects=True,
            timeout=aiohttp.ClientTimeout(total=10),
            proxy=Config.PROXY
        ) as head:
            mime = head.headers.get("Content-Type", "").split(";")[0].strip()
            cd = head.headers.get("Content-Disposition", "")
            # Check server-provided exact filename
            cd_match = re.search(r'filename="?([^"]+)"?', cd)
            if cd_match:
                return smart_output_name(cd_match.group(1))

            # If no Content-Disposition, check if the parsed URL name lacks an extension
            parsed_name = urllib.parse.unquote(os.path.basename(urllib.parse.urlparse(url).path.rstrip("/")))
            base_name = parsed_name if parsed_name else default_name
            
            if not os.path.splitext(base_name)[1]:
                ext = mimetypes.guess_extension(mime)
                if ext:
                    if ext == '.jpe': ext = '.jpg'
                    base_name += ext
                    
            return smart_output_name(base_name)
    except Exception:
        # Fallback to standard URL parsing
        parsed = urllib.parse.urlparse(url)
        name = os.path.basename(parsed.path.rstrip("/"))
        return smart_output_name(urllib.parse.unquote(name) if name else default_name)


async def get_best_filename(url: str, default_name: str = "downloaded_file") -> str:
    """
    Universally determine the best filename for any given URL.
    Routes to yt-dlp native extraction first, falling back to HTTP header sniffing for direct routes.
    """
    if is_ytdlp_url(url):
        ytdlp_title = await fetch_ytdlp_title(url)
        if ytdlp_title:
            return ytdlp_title
        # Even if it's a yt-dlp URL, if the title extraction fails (like on Pinterest), fall back
    
    # If it's a cobalt URL, Cobalt handles social media links so HTTP probes usually just return HTML.
    # So we don't bother probing Cobalt URLs, just return the parsed stem + default .mp4
    if is_cobalt_url(url):
        parsed = urllib.parse.urlparse(url)
        name = os.path.basename(parsed.path.rstrip("/"))
        base_name = urllib.parse.unquote(name) if name else default_name
        if not os.path.splitext(base_name)[1]:
            base_name += ".mp4"
        return smart_output_name(base_name)

    return await fetch_http_filename(url, default_name)


async def fetch_ytdlp_formats(url: str) -> dict:
    """
    Fetch available video formats from yt-dlp.
    Returns: {"formats": list[dict], "title": str}
    """
    if not YTDLP_AVAILABLE:
        return {"formats": [], "title": ""}

    # PRIMARY FOR YOUTUBE: Try external API first to bypass bot detection
    if "youtube.com" in url or "youtu.be" in url:
        Config.LOGGER.info(f"Primary YouTube format extraction via external API: {url}")
        info = await external_extract_youtube(url)
        if info:
            formats = info.get("formats", [])
            title = info.get("title", "video")

            best_audio_size = 0
            for f in formats:
                if f.get("vcodec") == "none" and f.get("acodec") != "none":
                    size = f.get("filesize") or f.get("filesize_approx") or 0
                    if size > best_audio_size:
                        best_audio_size = size

            available = {}
            for f in formats:
                height = f.get("height")
                if height and f.get("vcodec") != "none":
                    res = f"{height}p"
                    available[res] = f

            format_results = []
            sorted_res = sorted(
                available.keys(),
                key=lambda x: int(re.search(r'(\d+)p', x).group(1)) if re.search(r'(\d+)p', x) else 0,
                reverse=True
            )

            for res in sorted_res:
                f = available[res]
                size = f.get("filesize") or f.get("filesize_approx")
                
                # Estimate size if missing using bitrate and duration
                if size is None:
                    tbr = f.get("tbr")
                    duration = info.get("duration")
                    if tbr and duration:
                        size = int((tbr * 1024 / 8) * duration)

                if f.get("acodec") == "none" and size is not None and best_audio_size > 0:
                    size += best_audio_size

                format_results.append({
                    "format_id": f["format_id"],
                    "resolution": res,
                    "ext": f.get("ext", "mp4"),
                    "filesize": size,
                    "url": f.get("url")
                })

            if format_results:
                # ‚îÄ‚îÄ Final safety probe for missing YouTube filesizes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                session = await get_http_session()
                for f_dict in format_results:
                    if f_dict.get("filesize") is None and f_dict.get("url"):
                        try:
                            async with session.head(
                                f_dict["url"], allow_redirects=True, 
                                timeout=aiohttp.ClientTimeout(total=5),
                                proxy=Config.PROXY
                            ) as head:
                                cl = head.headers.get("Content-Length")
                                if cl and cl.isdigit():
                                    f_dict["filesize"] = int(cl)
                        except Exception:
                            pass
                    # Remove temporary URL before sending to client
                    if "url" in f_dict:
                        del f_dict["url"]
                
                return {"formats": format_results, "title": title}

    loop = asyncio.get_running_loop()

    def _fetch():
        try:
            opts = {
                "quiet": True,
                "no_warnings": True,
                "format_sort": ["res", "vbr", "tbr", "fps"],
                "force_ipv4": True,
                "nocheckcertificate": True, # Ignore SSL artifacts
                "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "extractor_args": {
                    "youtube": {
                        "player_client": ["web", "creator"],
                    },
                    "youtubepot-bgutilhttp": {
                        "base_url": ["http://localhost:4416"],
                    }
                }
            }


            if Config.COOKIES_FILE and os.path.exists(Config.COOKIES_FILE):
                opts["cookiefile"] = Config.COOKIES_FILE
            if Config.PROXY:
                opts["proxy"] = Config.PROXY
            
            # NSFW / PornHub tweaks
            if "pornhub.com" in url:
                opts["referer"] = "https://www.pornhub.com/"
                opts["geo_bypass"] = True
                opts["socket_timeout"] = 20
                opts["extractor_args"]["pornhub"] = {'prefer_formats': 'mp4'}

            with yt_dlp.YoutubeDL(opts) as ydl:
                info = ydl.extract_info(url, download=False)
                formats = info.get("formats", [])
                title = info.get("title", "video")
                
                # Find the best audio size to add to video-only stream sizes
                best_audio_size = 0
                for f in formats:
                    if f.get("vcodec") == "none" and f.get("acodec") != "none":
                        size = f.get("filesize") or f.get("filesize_approx") or 0
                        if size > best_audio_size:
                            best_audio_size = size

                # Filter for useful formats
                available = {}
                for f in formats:
                    height = f.get("height")
                    
                    # Facebook Missing Qualities Fix: HD/SD streams have height=None
                    if height is None:
                        fid = str(f.get("format_id", "")).lower()
                        if fid == "hd":
                            height = 720
                        elif fid == "sd":
                            height = 360
                    
                    if height and f.get("vcodec") != "none":
                        res = f"{height}p"
                        # yt-dlp returns formats sorted from worst to best.
                        # Overwriting the key guarantees we keep the best format for this resolution
                        available[res] = f
                
                results = []
                sorted_res = sorted(
                    available.keys(), 
                    key=lambda x: int(re.search(r'(\d+)p', x).group(1)) if re.search(r'(\d+)p', x) else 0, 
                    reverse=True
                )
                
                for res in sorted_res:
                    f = available[res]
                    size = f.get("filesize") or f.get("filesize_approx")
                    
                    # Estimate size if missing using bitrate and duration
                    if size is None:
                        tbr = f.get("tbr")
                        duration = info.get("duration")
                        if tbr and duration:
                            size = int((tbr * 1024 / 8) * duration)

                    # Add audio size only if we have a base video size and it lacks audio
                    if f.get("acodec") == "none" and size is not None and best_audio_size > 0:
                        size += best_audio_size
                        
                    results.append({
                        "format_id": f["format_id"],
                        "resolution": res,
                        "ext": f.get("ext", "mp4"),
                        "filesize": size,
                        "url": f.get("url")
                    })
                
                # If we only found 0 formats, we return empty list so the bot skips selection
                if len(results) < 1:
                    return {"formats": [], "title": title}
                    
                return {"formats": results, "title": title}
        except Exception as e:
            Config.LOGGER.error(f"Error fetching formats for {url}: {e}")
            return {"formats": [], "title": ""}

    res = await loop.run_in_executor(None, _fetch)
    
    # ‚îÄ‚îÄ Final safety probe for missing filesizes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if res and res.get("formats"):
        session = await get_http_session()
        for f_dict in res["formats"]:
            # If still None (estimation failed) and we have a direct stream URL, try HEAD
            if f_dict.get("filesize") is None and f_dict.get("url"):
                try:
                    # Short timeout to avoid blocking UI
                    async with session.head(
                        f_dict["url"], allow_redirects=True, 
                        timeout=aiohttp.ClientTimeout(total=5),
                        proxy=Config.PROXY
                    ) as head:
                        cl = head.headers.get("Content-Length")
                        if cl and cl.isdigit():
                            f_dict["filesize"] = int(cl)
                except Exception:
                    pass
            
            # Remove temporary URL before sending to client
            if "url" in f_dict:
                del f_dict["url"]

    return res

async def check_ffmpeg() -> bool:
    """Check if ffmpeg is available."""
    try:
        proc = await asyncio.create_subprocess_exec(
            _get_ffmpeg_bin(), "-version",
            stdout=asyncio.subprocess.DEVNULL,
            stderr=asyncio.subprocess.DEVNULL
        )
        await proc.wait()
        return proc.returncode == 0
    except Exception:
        return False


async def download_ytdlp(
    url: str,
    filename: str,
    progress_msg,
    start_time_ref: list,
    user_id: int,
    format_id: str = None,
    cancel_ref: list = None,
) -> tuple[str, str]:
    """
    Download content using yt-dlp with live progress.
    Uses the user-supplied filename as the output stem.
    If format_id is provided, it attempts to download that specific format.
    Pass cancel_ref=[False] to safely abort via asyncio.
    Returns (file_path, mime_type).
    """
    start_time_ref[0] = time.time()
    loop = asyncio.get_running_loop()
    last_edit = [start_time_ref[0]]

    out_dir = Config.DOWNLOAD_LOCATION
    os.makedirs(out_dir, exist_ok=True)
    
    Config.LOGGER.info(f"download_ytdlp started for {user_id}. SyncID={id(WEBAPP_PROGRESS)}")

    # State update for WebApp
    WEBAPP_PROGRESS[user_id] = {
        "action": "Analyzing Media URL...",
        "percentage": 5,
        "current": "0 B",
        "total": "Fetching size...",
        "speed": "---"
    }

    # Build a safe output stem from the user-chosen filename
    # Shorten to 80 chars to avoid OS length limits (e.g. for long Facebook titles)
    safe_stem = re.sub(r'[\\/*?"<>|:]', "_", os.path.splitext(filename)[0])[:80]
    if not safe_stem:
        safe_stem = "video_file"
    outtmpl = os.path.join(out_dir, f"{safe_stem}.%(ext)s")

    def _progress_hook(d: dict):
        if cancel_ref and cancel_ref[0]:
            raise asyncio.CancelledError("Download cancelled by user.")

        done = d.get("downloaded_bytes", 0)
        total = d.get("total_bytes") or d.get("total_bytes_estimate") or d.get("filesize") or d.get("filesize_approx", 0)
        speed = d.get("speed") or 0
        eta = d.get("eta") or 0
        percent = (done / total * 100) if total else 0

        # 1. Update global WebApp tracker FREQUENTLY
        # Cap at 99.5% to avoid the frontend thinking it's totally finished 
        # while yt-dlp might still be merging or finishing its internal loops.
        display_percent = max(1, round(percent, 1))
        if display_percent >= 100:
            display_percent = 99.5

        WEBAPP_PROGRESS[user_id] = {
            "action": "Downloading Media...",
            "current": humanbytes(done),
            "total": humanbytes(total) if total else "Unknown",
            "speed": f"{humanbytes(speed)}/s" if speed else "",
            "percentage": display_percent
        }

        # 2. Update Telegram message INFREQUENTLY
        now = time.time()
        if d["status"] == "downloading" and now - last_edit[0] >= PROGRESS_UPDATE_DELAY:
            last_edit[0] = now
            bar = progress_bar(done, total) if total else "‚ñë" * 12
            pct = f"{done / total * 100:.1f}%" if total else "‚Ä¶"
            text = (
                f"üì• **Downloading Media‚Ä¶**\n\n"
                f"üìÅ **Name:** `{os.path.basename(outtmpl)}`\n"
                f"[{bar}] {pct}\n"
                f"**Done:** {humanbytes(done)}"
                + (f" / {humanbytes(total)}" if total else "")
                + (f"\n**Speed:** {humanbytes(speed)}/s" if speed else "")
                + (f"\n**ETA:** {time_formatter(eta)}" if eta else "")
            )
            asyncio.run_coroutine_threadsafe(
                _safe_edit(progress_msg, text, reply_markup=cancel_button(user_id)),
                loop
            )

    # ‚îÄ‚îÄ Build format string based on ffmpeg availability ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ffmpeg_available = await check_ffmpeg()

    if format_id == "best":
        # User requested absolute max quality (unrestricted resolution)
        if ffmpeg_available:
            fmt = "bestvideo+bestaudio/best"
        else:
            fmt = "best"
    elif format_id:
        # If user picked a specific resolution, we try to get that video + best audio
        # or just that specific format if it's already merged.
        if ffmpeg_available:
            fmt = f"{format_id}+bestaudio/{format_id}/best"
        else:
            fmt = f"{format_id}/best"
    elif ffmpeg_available:
        # ffmpeg found ‚Üí prefer best quality separate streams, merge to mp4
        fmt = (
            "bestvideo[height<=1080][ext=mp4]+bestaudio[ext=m4a]"
            "/bestvideo[height<=1080]+bestaudio"
            "/best[height<=1080]/best"
        )
    else:
        # NO ffmpeg ‚Üí only pick formats that are already a single file (no merge needed)
        fmt = (
            "best[height<=1080][ext=mp4]"
            "/best[height<=1080]"
            "/best"
        )

    ydl_opts = {
        "format": fmt,
        "format_sort": [
            "res",        # Prefer highest resolution (absolute best)
            "vbr",        # Prefer highest video bitrate
            "tbr",        # Prefer highest total bitrate
            "fps",        # Prefer highest frame rate
        ],
        "outtmpl": outtmpl,
        "progress_hooks": [_progress_hook],
        "quiet": True,
        "no_warnings": True,
        "force_ipv4": True,
        "nocheckcertificate": True,
        "cookiefile": Config.COOKIES_FILE,
        "merge_output_format": "mp4",
        "overwrites": True,
        "noplaylist": True,
        "max_filesize": Config.MAX_FILE_SIZE,
        "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "concurrent_fragment_downloads": 10, # Reduced for improved stability on low-resource hosts
        "hls_prefer_native": True,          # Native HLS allows better progress reporting
        "cachedir": False,                   # Disable cache to avoid path/permission issues on Koyeb
        "trim_file_name": 100,              # Prevent Errno 2 by ensuring total path length stays safe
        "retries": 15,
        "fragment_retries": 15,
        "buffersize": 1048576,               # 1MB Buffer for speed
        "extractor_args": {
            "youtube": {
                "player_client": ["web", "creator"],
            },
            "youtubepot-bgutilhttp": {
                "base_url": ["http://localhost:4416"],
            }
        }
    }

    # Additional logic for NSFW/Blocked sites
    if "pornhub.com" in url.lower():
        ydl_opts["http_headers"] = {"Referer": "https://www.pornhub.com/"}
        ydl_opts["extractor_args"]["pornhub"] = {'prefer_formats': 'mp4'}

    # Set ffmpeg_location to the DIRECTORY, not the binary path
    ffmpeg_dir = _get_ffmpeg_dir()
    if ffmpeg_dir:
        ydl_opts["ffmpeg_location"] = ffmpeg_dir

    if Config.COOKIES_FILE and os.path.exists(Config.COOKIES_FILE):
        ydl_opts["cookiefile"] = Config.COOKIES_FILE

    if Config.PROXY:
        ydl_opts["proxy"] = Config.PROXY

    # Platform specific tweaks
    if "reddit.com" in url or "v.redd.it" in url:
        ydl_opts["referer"] = "https://www.reddit.com/"
    elif "pornhub.com" in url:
        ydl_opts["referer"] = "https://www.pornhub.com/"
        ydl_opts["geo_bypass"] = True
        ydl_opts["socket_timeout"] = 30
        ydl_opts["extractor_args"] = {'pornhub': {'prefer_formats': 'mp4'}}

    async def _run_async() -> str:
        try:
            # PRIMARY FOR YOUTUBE: Try external API first during download exploration too
            cached_info = None
            if "youtube.com" in url or "youtu.be" in url:
                Config.LOGGER.info(f"Primary YouTube download extraction via external API: {url}")
                cached_info = await external_extract_youtube(url)

            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                # 1. Extract metadata only
                if cached_info:
                    info = cached_info
                    # If external API provides a specific format for the requested resolution, try to re-process it locally
                    # Note: process_info will handle the download.
                else:
                    try:
                        info = await loop.run_in_executor(None, lambda: ydl.extract_info(url, download=False))
                    except Exception as e:
                        # FALLBACK: If local extraction fails for YouTube during download phase
                        if "youtube.com" in url or "youtu.be" in url:
                            Config.LOGGER.info(f"Local download-phase extraction failed for YouTube. Trying external API: {url}")
                            info = await external_extract_youtube(url)
                            if not info:
                                raise e 
                        else:
                            raise e

                # Use safe_stem to build path (we prefer the user's filename)
                ext = info.get("ext", "mp4")
                target_path = os.path.join(out_dir, f"{safe_stem}.{ext}")
                
                # 2. Identify if handover to aria2c is possible (single direct file)
                req_formats = info.get("requested_formats")
                is_single = not req_formats or len(req_formats) == 1
                protocol = info.get("protocol", "")
                
                extractor = info.get("extractor_key", "").lower()
                tricky_extractors = ["facebook", "instagram", "twitter", "x", "tiktok"]
                is_tricky = any(t in extractor for t in tricky_extractors)
                
                is_direct = "http" in protocol and "m3u8" not in protocol and "dash" not in protocol and not is_tricky

                downloaded_path = None
                if is_single and is_direct:
                    Config.LOGGER.info(f"Handing off direct URL to aria2c for max speed: {url}")
                    headers = info.get("http_headers", {})
                    downloaded_path = await _download_aria2c(
                        info["url"], target_path, progress_msg, start_time_ref, user_id, 
                        cancel_ref=cancel_ref, 
                        headers=headers
                    )
                else:
                    # 3. Fallback: Use yt-dlp native downloader
                    Config.LOGGER.info(f"Using native yt-dlp downloader for complex stream/merge: {url}")
                    await loop.run_in_executor(None, lambda: ydl.process_info(info))
                    
                    # Determine output path
                    mp4_path = os.path.join(out_dir, f"{safe_stem}.mp4")
                    if os.path.exists(mp4_path):
                        downloaded_path = mp4_path
                    else:
                        candidates = sorted(
                            [f for f in os.listdir(out_dir) if f.startswith(safe_stem)],
                            key=lambda f: os.path.getsize(os.path.join(out_dir, f)),
                            reverse=True,
                        )
                        if candidates:
                            downloaded_path = os.path.join(out_dir, candidates[0])
                
                if not downloaded_path or not os.path.exists(downloaded_path):
                    raise FileNotFoundError("Error: output file not found after download")

                # CRITICAL: Fix 0B file issue
                if os.path.getsize(downloaded_path) == 0:
                    try:
                        os.remove(downloaded_path)
                    except:
                        pass
                    raise ValueError("Downloaded file is empty (0 bytes). The download probably failed silently or was blocked.")

                return downloaded_path

        except Exception as e:
            if isinstance(e, asyncio.CancelledError):
                raise
            Config.LOGGER.error(f"yt-dlp/aria2c critical error for {url}: {e}")
            raise

    file_path = await _run_async()
    mime = mimetypes.guess_type(file_path)[0] or "video/mp4"
    return file_path, mime


# ‚îÄ‚îÄ Cobalt API fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def download_cobalt(
    url: str,
    filename: str,
    progress_msg,
    start_time_ref: list,
    user_id: int,
    cancel_ref: list = None,
) -> tuple[str, str]:
    """
    Download content using the cobalt API (fallback for Instagram/Pinterest).
    No cookies required ‚Äî cobalt handles authentication independently.
    Returns (file_path, mime_type).
    """
    start_time_ref[0] = time.time()
    out_dir = Config.DOWNLOAD_LOCATION
    os.makedirs(out_dir, exist_ok=True)
    safe_stem = re.sub(r'[\\/*?"<>|:]', "_", os.path.splitext(filename)[0])[:80]

    api_url = Config.COBALT_API_URL.rstrip("/")
    payload = {
        "url": url,
        "downloadMode": "auto",
        "videoQuality": "1080",
        "filenameStyle": "basic",
    }
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "User-Agent": "TelegramBot/1.0",
    }

    try:
        await _safe_edit(
            progress_msg,
            "üì• **Initializing Download‚Ä¶** ‚è≥\n_Please wait while we prepare your file..._",
            reply_markup=cancel_button(user_id)
        )
        
        # Initial state for WebApp
        WEBAPP_PROGRESS[user_id] = {
            "action": "Requesting Extraction Server...",
            "percentage": 5,
            "current": "0 B",
            "total": "Unknown",
            "speed": "---"
        }

        session = await get_http_session()
        # Step 1: Ask cobalt for the download URL
        async with session.post(
            f"{api_url}/",
            json=payload,
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=30),
            proxy=Config.PROXY
        ) as resp:
            if resp.status != 200:
                error_text = await resp.text()
                raise ValueError(f"Download server returned {resp.status}: {error_text[:200]}")
            data = await resp.json()

            status = data.get("status")

            if status == "error":
                error_code = data.get("error", {}).get("code", "unknown")
                raise ValueError(f"Extraction error: {error_code}")

            # Get the download URL from the response
            if status in ("tunnel", "redirect"):
                download_url_str = data.get("url")
                cobalt_filename = data.get("filename", f"{safe_stem}.mp4")
            elif status == "picker":
                # Multiple items ‚Äî take the first video/photo
                picker = data.get("picker", [])
                if not picker:
                    raise ValueError("No media found to extract")
                download_url_str = picker[0].get("url")
                cobalt_filename = f"{safe_stem}.mp4"
            else:
                raise ValueError(f"Download server returned unexpected status: {status}")

            if not download_url_str:
                raise ValueError("Could not extract media URL")

            # Determine output file extension from cobalt filename
            _, ext = os.path.splitext(cobalt_filename)
            if not ext:
                ext = ".mp4"
            out_path = os.path.join(out_dir, f"{safe_stem}{ext}")

            try:
                # Step 2: Download extremely fast via aria2c using the Cobalt proxy URL
                await _safe_edit(progress_msg, "üì• **Extracting Media‚Ä¶** ‚öôÔ∏è", reply_markup=cancel_button(user_id))
                
                # Transition state for WebApp
                WEBAPP_PROGRESS[user_id] = {
                    "action": "Starting Download...",
                    "percentage": 10,
                    "current": "0 B",
                    "total": "Fetching...",
                    "speed": "Waiting"
                }

                await _download_aria2c(download_url_str, out_path, progress_msg, start_time_ref, user_id, cancel_ref=cancel_ref)

            except Exception:
                if os.path.exists(out_path):
                    try:
                        os.remove(out_path)
                    except Exception:
                        pass
                raise

            if not os.path.exists(out_path) or os.path.getsize(out_path) == 0:
                try:
                    os.remove(out_path)
                except:
                    pass
                raise ValueError("Downloaded file from secondary server is empty (0 bytes).")

            mime = mimetypes.guess_type(out_path)[0] or "video/mp4"
            return out_path, mime

    except Exception as e:
        raise ValueError(f"Download failed: {e}")


def humanbytes(size: int) -> str:
    if size is None or size < 0:
        return "Unknown"
    if not size:
        return "0 B"
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if size < 1024:
            if unit in ["B", "KB"]:
                return f"{int(size)} {unit}"
            return f"{size:.2f} {unit}"
        size /= 1024
    return f"{size:.2f} PB"


def time_formatter(seconds: float) -> str:
    minutes, sec = divmod(int(seconds), 60)
    hours, minutes = divmod(minutes, 60)
    if hours:
        return f"{hours}h {minutes}m {sec}s"
    elif minutes:
        return f"{minutes}m {sec}s"
    return f"{sec}s"


def progress_bar(current: int, total: int, length: int = 12) -> str:
    filled = int(length * current / total) if total else 0
    bar = "‚ñà" * filled + "‚ñë" * (length - filled)
    percent = current / total * 100 if total else 0
    return f"[{bar}] {percent:.1f}%"


# ‚îÄ‚îÄ FFprobe / FFmpeg helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def get_video_metadata(file_path: str) -> dict:
    """
    Use ffprobe (async subprocess) to extract duration, width, height from a video.
    Returns a dict with keys: duration (int seconds), width (int), height (int).
    Falls back to zeros if ffprobe is unavailable or fails.
    """
    try:
        proc = await asyncio.create_subprocess_exec(
            _get_ffprobe_bin(),
            "-v", "quiet",
            "-print_format", "json",
            "-show_streams",
            "-show_format",
            file_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=60)
        data = json.loads(stdout)
        video_stream = next(
            (s for s in data.get("streams", []) if s.get("codec_type") == "video"),
            None,
        )
        duration = int(float(data.get("format", {}).get("duration", 0)))
        width = int(video_stream.get("width", 0)) if video_stream else 0
        height = int(video_stream.get("height", 0)) if video_stream else 0
        return {"duration": duration, "width": width, "height": height}
    except Exception:
        return {"duration": 0, "width": 0, "height": 0}


async def generate_video_thumbnail(file_path: str, chat_id: int, duration: int = 0) -> str | None:
    """
    Extract a single frame from the video at 10% of its duration (or 1 s if unknown),
    scaled to max width 320 px, saved as JPEG.  Returns the path or None on failure.
    """
    thumb_path = os.path.join(Config.DOWNLOAD_LOCATION, f"thumb_auto_{chat_id}.jpg")
    # Pick a timestamp: 0 seconds (first frame) to avoid any seeking overhead and instantly grab the screen
    seek = 0
    try:
        proc = await asyncio.create_subprocess_exec(
            _get_ffmpeg_bin(),
            "-y",
            "-threads", "1",
            "-ss", str(seek),
            "-i", file_path,
            "-vframes", "1",
            "-vf", "scale=320:-1",
            "-q:v", "2",          # JPEG quality (2 = very high, 31 = worst)
            thumb_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        await asyncio.wait_for(proc.communicate(), timeout=60)
        if os.path.exists(thumb_path) and os.path.getsize(thumb_path) > 0:
            return thumb_path
    except Exception:
        pass
    return None


# ‚îÄ‚îÄ Download helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def _download_hls(url: str, out_path: str, progress_msg, start_time_ref: list, user_id: int, cancel_ref: list = None) -> str:
    """
    Use ffmpeg to download an HLS/DASH/TS stream and remux it to mp4.
    Shows elapsed-time progress (no size info available for streams).
    """
    start_time_ref[0] = time.time()
    last_edit = start_time_ref[0]

    proc = await asyncio.create_subprocess_exec(
        _get_ffmpeg_bin(), "-y",
        "-i", url,
        "-c", "copy",
        "-bsf:a", "aac_adtstoasc",   # fix AAC bitstream for mp4 container
        out_path,
        stdout=asyncio.subprocess.DEVNULL,
        stderr=asyncio.subprocess.PIPE,
    )

    # Read stderr in background so pipe doesn't fill up and block ffmpeg
    stderr_chunks = []

    async def _read_stderr():
        while True:
            line = await proc.stderr.readline()
            if not line:
                break
            stderr_chunks.append(line)

    stderr_task = asyncio.create_task(_read_stderr())

    # Poll until ffmpeg finishes, editing progress every PROGRESS_UPDATE_DELAY s
    try:
        while True:
            try:
                await asyncio.wait_for(proc.wait(), timeout=1.0)
                break  # process finished
            except asyncio.TimeoutError:
                pass  # still running, update progress
            
            if cancel_ref and cancel_ref[0]:
                try:
                    proc.kill()
                except Exception:
                    pass
                raise asyncio.CancelledError("Upload cancelled.")

            now = time.time()
            if now - last_edit >= PROGRESS_UPDATE_DELAY:
                elapsed = now - start_time_ref[0]
                # Since HLS streaming size is unknown, we fake a pulsing progress bar in the UI
                pulsing_pct = min(((elapsed % 10) / 10) * 100, 99.9)
                WEBAPP_PROGRESS[user_id] = {
                    "action": "Weaving Stream together... üßµ",
                    "current": time_formatter(elapsed),
                    "total": "Unknown",
                    "speed": "Streaming",
                    "percentage": round(pulsing_pct, 1)
                }
                try:
                    await progress_msg.edit_text(
                        f"üì• **Weaving the stream together‚Ä¶** üßµ\n"
                        f"‚è± Elapsed: {time_formatter(elapsed)}",
                        reply_markup=cancel_button(user_id)
                    )
                except Exception:
                    pass
                last_edit = now

        await stderr_task  # ensure stderr is fully read

        if proc.returncode != 0:
            err_log = b"".join(stderr_chunks).decode(errors="replace")
            raise RuntimeError(f"ffmpeg stream download failed:\n{err_log[-600:] if err_log else 'Unknown error'}")

        return out_path
    except Exception:
        # Cleanup incomplete ffmpeg output if cancelled or failed
        if os.path.exists(out_path):
            try:
                os.remove(out_path)
            except Exception:
                pass
        raise
    finally:
        if proc.returncode is None:
            try:
                proc.kill()
            except Exception:
                pass


async def download_url(url: str, filename: str, progress_msg, start_time_ref: list, user_id: int, format_id: str = None, cancel_ref: list = None):
    """
    Stream-download a URL to disk, editing progress_msg periodically.
    Returns (path, mime_type) on success or raises.
    """
    download_dir = Config.DOWNLOAD_LOCATION
    os.makedirs(download_dir, exist_ok=True)

    # Remap streaming extensions to proper container (e.g. .m3u8 ‚Üí .mp4)
    filename = smart_output_name(filename)
    # Shorten to 80 chars to avoid OS length limits
    safe_name = re.sub(r'[\\/*?:"<>|]', "_", filename)[:80]
    file_path = os.path.join(download_dir, safe_name)

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5"
    }

    # ‚îÄ‚îÄ Route yt-dlp-supported platforms ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if is_ytdlp_url(url):
        try:
            status_text = "üì• **Doing some black magic‚Ä¶** ü™Ñ\n_(connecting to the dark side‚Ä¶)_"
            await progress_msg.edit_text(status_text, reply_markup=cancel_button(user_id))
        except Exception:
            pass
        try:
            res_path, res_mime = await download_ytdlp(url, filename, progress_msg, start_time_ref, user_id, format_id=format_id, cancel_ref=cancel_ref)
            
            # Final guard against 0B files leaking to uploader
            if res_path and os.path.exists(res_path) and os.path.getsize(res_path) > 0:
                return res_path, res_mime
            else:
                raise ValueError("Final yt-dlp download produced a 0B file.")

        except Exception as ytdlp_err:
            if isinstance(ytdlp_err, asyncio.CancelledError):
                raise
            
            # If yt-dlp fails and cobalt supports this URL, try cobalt as fallback
            if is_cobalt_url(url):
                Config.LOGGER.info(
                    "Initial extraction failed, trying secondary servers..."
                )
                try:
                    return await download_cobalt(url, filename, progress_msg, start_time_ref, user_id, cancel_ref=cancel_ref)
                except Exception as cobalt_err:
                    # Both failed ‚Äî raise the original yt-dlp error with cobalt context
                    raise ValueError(
                        f"Error 1: {ytdlp_err}\n\nError 2: {cobalt_err}"
                    ) from ytdlp_err
            else:
                # If neither yt-dlp nor Cobalt succeeded, raise immediately.
                # DO NOT fall through to the raw HTTP generic downloader, solving the "0 B file" issue.
                raise ValueError(str(ytdlp_err)) from ytdlp_err

    # Secondary extraction route: Force Cobalt for skipped yt-dlp domains (e.g. YouTube)
    if is_cobalt_url(url):
        try:
            return await download_cobalt(url, filename, progress_msg, start_time_ref, user_id, cancel_ref=cancel_ref)
        except Exception:
            pass # fall through to aria2c/http probe if cobalt fails

    # Transition state for WebApp
    WEBAPP_PROGRESS[user_id] = {
        "action": "Identifying Resource...",
        "percentage": 5,
        "current": "0 B",
        "total": "Checking...",
        "speed": "---"
    }

    # ‚îÄ‚îÄ Probe the URL to detect content type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    session = await get_http_session()
    async with session.head(
        url, allow_redirects=True,
        timeout=aiohttp.ClientTimeout(total=30),
        proxy=Config.PROXY
    ) as head:
        mime = head.headers.get("Content-Type", "").split(";")[0].strip()
        total_str = head.headers.get("Content-Length", "0")
        total = int(total_str) if total_str.isdigit() else 0
        
        # Extract true filename if available from server
        cd = head.headers.get("Content-Disposition", "")
        cd_match = re.search(r'filename="?([^"]+)"?', cd)
        if cd_match:
            filename = cd_match.group(1)
        else:
            # If no Content-Disposition, and filename lacks an extension, guess via mime
            if not os.path.splitext(filename)[1]:
                ext = mimetypes.guess_extension(mime)
                if ext:
                    # Some systems return '.jpe' for jpeg
                    if ext == '.jpe': ext = '.jpg'
                    filename += ext

    # Re-evaluate safe filename based on true network name
    filename = smart_output_name(filename)
    safe_name = re.sub(r'[\\/*?:"<>|]', "_", filename)[:80]
    file_path = os.path.join(download_dir, safe_name)

    # Transition state for WebApp
    WEBAPP_PROGRESS[user_id] = {
        "action": "Starting Stream Download...",
        "percentage": 10,
        "current": "0 B",
        "total": humanbytes(total) if total else "Unknown",
        "speed": "---"
    }

    # ‚îÄ‚îÄ Route HLS / DASH / TS streams through ffmpeg ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if needs_ffmpeg_download(url, mime):
        # Force mp4 output path
        mp4_path = os.path.splitext(file_path)[0] + ".mp4"
        try:
            await progress_msg.edit_text(
                "üì• **Downloading stream‚Ä¶**\n"
                "_(live stream detected ‚Äî stitching it together‚Ä¶)_ üßµ",
                reply_markup=cancel_button(user_id)
            )
        except Exception:
            pass
        await _download_hls(url, mp4_path, progress_msg, start_time_ref, user_id, cancel_ref=cancel_ref)
        return mp4_path, "video/mp4"

    # ‚îÄ‚îÄ Aria2c High Speed Download ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if total > Config.MAX_FILE_SIZE:
        raise ValueError(
            f"File too large: {humanbytes(total)} (max {humanbytes(Config.MAX_FILE_SIZE)})"
        )

    await _download_aria2c(url, file_path, progress_msg, start_time_ref, user_id, cancel_ref=cancel_ref)

    mime_from_ext = mimetypes.guess_type(file_path)[0]
    final_mime = mime_from_ext or mime
    return file_path, final_mime



# ‚îÄ‚îÄ Aria2c Custom Downloader ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Global aria2 client bound to the daemon we started in bot.py
aria2 = aria2p.API(
    aria2p.Client(
        host="http://localhost",
        port=6800,
        secret=""
    )
)

async def _download_aria2c(url: str, out_path: str, progress_msg, start_time_ref: list, user_id: int, cancel_ref: list = None, headers: dict = None) -> str:
    """
    Download extremely fast using aria2c native RPC daemon.
    Uses 16 concurrent HTTP streams and no file allocation for Koyeb disk stability.
    """
    start_time_ref[0] = time.time()
    last_edit = start_time_ref[0]
    
    # Ensure background aria2c daemon writes exactly where we expect
    out_path = os.path.abspath(out_path)
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    options = {
        "max-connection-per-server": "16",
        "split": "16",
        "min-split-size": "1M",
        "file-allocation": "none",
        "dir": os.path.dirname(out_path),
        "out": os.path.basename(out_path),
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "header": ["Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8"]
    }
    
    # Inject global cookies directly into aria2c so it can authenticate social media direct links just like yt-dlp
    if os.path.exists(Config.COOKIES_FILE):
        options["load-cookies"] = os.path.abspath(Config.COOKIES_FILE)

    # Merge custom headers (e.g. Cookies, Referer from yt-dlp)
    if headers:
        if "header" not in options:
            options["header"] = []
        for k, v in headers.items():
            # aria2 expects "Name: Value" strings in the header list
            options["header"].append(f"{k}: {v}")
        if "User-Agent" in headers:
            options["user-agent"] = headers["User-Agent"]

    # Add the download to the daemon
    download = await asyncio.to_thread(aria2.add_uris, [url], options)

    try:
        while True:
            await asyncio.to_thread(download.update)

            if download.is_complete:
                break

            if cancel_ref and cancel_ref[0]:
                await asyncio.to_thread(download.remove, force=True, files=True)
                raise asyncio.CancelledError("Download cancelled.")
            
            if download.has_failed:
                error_msg = download.error_message
                await asyncio.to_thread(download.remove, force=True, files=True)
                raise RuntimeError(f"aria2c download failed: {error_msg}")

            pct_int = int(download.progress)
            _bar = progress_bar(pct_int, 100)
            
            speed_str = download.download_speed_string()
            current_str = download.completed_length_string()
            total_str = download.total_length_string()

            # 1. Update global WebApp tracker FREQUENTLY (every 200ms)
            # Cap at 99.5% to avoid jumping to "Complete" before merging/uploading
            display_pct = pct_int
            if display_pct >= 100:
                display_pct = 99.5

            WEBAPP_PROGRESS[user_id] = {
                "action": "Downloading...",
                "current": current_str,
                "total": total_str,
                "speed": speed_str,
                "percentage": display_pct
            }

            # 2. Update Telegram message INFREQUENTLY (every 1s) to avoid flood
            now = time.time()
            if now - last_edit >= PROGRESS_UPDATE_DELAY:
                text = (
                    f"üì• **Downloading Media‚Ä¶** ‚¨áÔ∏è\n\n"
                    f"üìÅ **Name:** `{os.path.basename(out_path)}`\n"
                    f"[{_bar}] {download.progress_string()}\n"
                    f"**Done:** {current_str} / {total_str}\n"
                    f"**Speed:** {speed_str}\n"
                    f"**ETA:** {download.eta_string()}"
                )
                try:
                    await progress_msg.edit_text(text, reply_markup=cancel_button(user_id))
                except Exception:
                    pass
                last_edit = now

            await asyncio.sleep(0.2) # High frequency polling for smooth UI

        return out_path
    except Exception:
        if os.path.exists(out_path):
            try:
                os.remove(out_path)
            except Exception:
                pass
        raise



# ‚îÄ‚îÄ Upload helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def upload_file(
    client: Client,
    chat_id: int,
    file_path: str,
    mime: str,
    caption: str,
    thumb_file_id: str | None,
    progress_msg,
    start_time_ref: list,
    user_id: int,              # Explicit user_id for WEBAPP_PROGRESS tracking
    force_document: bool = False,
    cancel_ref: list = None,
):
    """
    Upload a local file to Telegram with:
    - Live progress bar
    - Correct duration / width / height for videos (extracted via ffprobe)
    - Auto-generated thumbnail from the video frame if no custom thumb is set
    - Custom thumbnail (downloaded from Telegram by file_id) if set by user
    """

    last_edit = [time.time()]
    start_time_ref[0] = time.time()

    # Immediate update for WebApp to indicate transition to Upload phase
    WEBAPP_PROGRESS[user_id] = {
        "action": "Uploading to Telegram...",
        "current": "0 B",
        "total": "Calculating...",
        "speed": "---",
        "percentage": 0.1
    }

    async def _progress(current: int, total: int):
        if cancel_ref and cancel_ref[0]:
            raise asyncio.CancelledError("Upload cancelled.")
            
        now = time.time()
        done = current
        percent = (done / total * 100) if total else 0
        elapsed = now - start_time_ref[0]
        speed = done / elapsed if elapsed else 0
        eta = (total - done) / speed if speed else 0
        
        # 1. Update global WebApp tracker FREQUENTLY
        WEBAPP_PROGRESS[user_id] = {
            "action": "Uploading to Telegram...",
            "current": humanbytes(done),
            "total": humanbytes(total) if total else "Unknown",
            "speed": f"{humanbytes(speed)}/s" if speed else "",
            "percentage": round(percent, 1)
        }

        # 2. Update Telegram message INFREQUENTLY
        if now - last_edit[0] < PROGRESS_UPDATE_DELAY:
            return
            
        bar = progress_bar(done, total)
        text = (
            "üì§ **Uploading‚Ä¶**\n\n"
            f"üìÅ **Name:** `{os.path.basename(file_path)}`\n"
            f"{bar}\n"
            f"**Done:** {humanbytes(done)} / {humanbytes(total)}\n"
            f"**Speed:** {humanbytes(speed)}/s\n"
            f"**ETA:** {time_formatter(eta)}"
        )
        try:
            await progress_msg.edit_text(text, reply_markup=cancel_button(chat_id))
        except Exception:
            pass
        last_edit[0] = now

    os.makedirs(Config.DOWNLOAD_LOCATION, exist_ok=True)
    is_video = not force_document and bool(mime and mime.startswith("video/"))
    is_audio = not force_document and bool(mime and mime.startswith("audio/"))
    is_image = not force_document and bool(mime and mime.startswith("image/"))

    # ‚îÄ‚îÄ 1. Get video metadata (duration, width, height) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    meta = {"duration": 0, "width": 0, "height": 0}
    if is_video:
        try:
            await progress_msg.edit_text("üîç Reading video metadata‚Ä¶", reply_markup=cancel_button(chat_id))
        except Exception:
            pass
        meta = await get_video_metadata(file_path)

    # Truncate caption to Telegram limit (1024)
    if caption and len(caption) > 1000:
        caption = caption[:997] + "..."

    # ‚îÄ‚îÄ 2. Resolve thumbnail ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Use a unique thumb name to avoid conflicts during concurrent uploads
    thumb_suffix = abs(hash(file_path)) % 10000
    thumb_local = None

    if thumb_file_id:
        try:
            # First, download to a generic path to see what extension Telegram gives us
            temp_thumb = await client.download_media(
                thumb_file_id,
                file_name=os.path.join(Config.DOWNLOAD_LOCATION, f"raw_thumb_{chat_id}_{thumb_suffix}"),
            )
            if temp_thumb and os.path.exists(temp_thumb):
                # Now normalize it with Pillow: Resize to 320x320 (proportional) and convert to JPEG
                normalized_path = os.path.join(Config.DOWNLOAD_LOCATION, f"thumb_user_{chat_id}_{thumb_suffix}.jpg")
                try:
                    with Image.open(temp_thumb) as img:
                        img.thumbnail((320, 320))
                        if img.mode != 'RGB':
                            img = img.convert('RGB')
                        img.save(normalized_path, "JPEG", quality=85, optimize=True)
                    thumb_local = normalized_path
                except Exception as img_err:
                    Config.LOGGER.error(f"Thumbnail normalization failed for {chat_id}: {img_err}")
                finally:
                    # Cleanup the raw download
                    if os.path.exists(temp_thumb):
                        os.remove(temp_thumb)
        except Exception as dl_err:
            Config.LOGGER.error(f"Custom thumbnail download failed for {chat_id}: {dl_err}")
            thumb_local = None

    if not thumb_local and is_video:
        try:
            await progress_msg.edit_text("üñºÔ∏è Generating fast thumbnail‚Ä¶", reply_markup=cancel_button(chat_id))
        except Exception:
            pass
        # Ensure duration is at least 1s for better thumbnail compatibility
        v_duration = max(1, meta["duration"])
        thumb_local = await generate_video_thumbnail(file_path, f"{chat_id}_{thumb_suffix}", v_duration)

    # ‚îÄ‚îÄ 3. Build kwargs (chat_id and file passed as positional args) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    kwargs = dict(
        caption=caption,
        parse_mode=None,
        progress=_progress,
    )
    if thumb_local:
        kwargs["thumb"] = thumb_local

    # ‚îÄ‚îÄ 4. Send to Telegram ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        # Final safety checks for metadata types
        v_duration = int(meta.get("duration", 0))
        v_width = int(meta.get("width", 0))
        v_height = int(meta.get("height", 0))

        if force_document:
            await client.send_document(chat_id, file_path, **kwargs)
        elif is_video:
            await client.send_video(
                chat_id,
                file_path,
                duration=v_duration,
                width=v_width,
                height=v_height,
                supports_streaming=True,
                **kwargs,
            )
        elif is_audio:
            await client.send_audio(chat_id, file_path, **kwargs)
        elif is_image:
            # üö® FIX: Pyrogram Photo messages DO NOT support thumbnails. 
            # If thumb_local is set, we MUST NOT pass it to send_photo.
            await client.send_photo(chat_id, file_path, caption=caption, progress=_progress)
        else:
            await client.send_document(chat_id, file_path, **kwargs)
    except Exception as e:
        Config.LOGGER.error(f"Critical Pyrogram send error for {file_path}: {e}")
        # Log more info to help debug serialization issues
        Config.LOGGER.error(f"Metadata: {meta}, Thumb: {thumb_local}, Caption Len: {len(caption) if caption else 0}, Mime: {mime}")
        raise
    finally:
        # Clean up any temp thumbnail files
        if thumb_local and os.path.exists(thumb_local):
            try:
                os.remove(thumb_local)
            except Exception:
                pass
